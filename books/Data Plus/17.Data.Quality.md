# Data Quality

## Common Data Quality Issues
- Blanks
- Nulls
- Outliers
- Duplicates
- Extra spaces
- Misspellings
- Abbreviations
- Formula errors

## Data Quality Challenges

- Duplicates
    - data representing the same transaction is duplicated in the system
    - human error, entering the same records into the system
- Redundant Data
    - the same data elements existing in multiple places
    - a result of integrating multiple data sources that make updates to their own data
    - inappropriate database design, without normalization, is also a culprit
- Missing Values
    - when you expect an attribute to contain data but nothing is there
    - can create calculations problems
- Invalid Data
    - outside the range of acceptable values
    - violate business rules, not technical rules
    - text data can have invalid character data in the absence of referential integrity, if two tables are related but have no foreign keys
- Nonparametric Data
    - data as categorical variables, which sometimes represent differentiation and sometimes rank order
- Data Outliers
    - a value that differs significantly from the rest
    - best to understand why they exist and whether they are valid
- Specification Mismatch
    - a value that doesn't meet business requirements
    - like trying to load text data into a numeric column
- Data Type Validation, ensures values in dataset have a consistent data type

## Quality Checks

- Data Acquisition
    - source systems may have inconsistencies
- Data Transformation and Conversion
    - when data is transformed or converted the chance for errors increase
- Data Manipulation
    - when data is reshaped during the ETL process
- Final Product
    - Software like Tableau can make bad joins if the analyst doesn't know the underlying data systems

## Automated Validation

- Automating data types before passing data into the system can prevent errors
- Data Validation, checks format and structure of the data
- Data Verification, checks the accuracy of the data

## Quality Dimensions

- Data Completeness, not missing data
    - the minimum about of information you need to fulfill a business objective
- Data Accuracy, data is correct
    - how closely the attribute matches its intended use
- Data Consistency, data format is consistent
    - the reliability of an attribute as it exists across multiple systems
- Data Timeliness
    - measures whether the data you need is available at the right time
- Data Conformity, data follows a set of standard data definitions
- Data Uniqueness
    - whether or not a data attribute exists in multiple places within the organization
    - the more unique the data is the less you worry about replication and consistency problems
- Data Validity, or data integrity
    - whether the data is within an expected range
    - one way to enforce this is referential integrity in the database

## Quality Rules and Metrics

- Organizations have different requirements for the data they work with
- Metrics, like how a mailing address might not require a person's name as long as the address is correct
- Rules, like how states may be recorded as "MN" instead of "Minnesota" depending on the rules of the business
- Data Cardinality, maximum number of times an instance in one entity can relate to instances of another
- Data Ordinality, minimum number of times an instance in one entity can relate to instances of another

## Methods to Validate Quality

- Reasonable Expectations
    - if a system generates x number of records a day, expect 30 * x monthly
    - eyeballing results
- Data Profiling
    - uses statistical methods to check results against the expectations
- Cross Validation
    - checking or comparing data between two systems
    - also, dividing data into two subsets to evaluate predictive model performance
- Peer Review
    - having another person check the data/analysis
- Data Audits, formal data governance
    - uses data profiling techniques to identify data integrity and security issues
- Sampling
    - subsetting data to make informed conclusions about the overall data

## Automated Validation

- Field Level Data, make sure actual data is correct
- Record Count, compare the record counts with data to ensure they match
- Calculations, verify the tools used to create calculations
    - the output of calculation
    - formula and equation are correct
- Report Visuals, double check graphs, captions, and columns
- Avoid visual inconsistencies in the visual reports

## Master Data Management

- Master Data, golden record or the master file for dimensional data types
- Master Data Management, refers to how we maintain the master data
- Reltio or Informatica are software packages that can be used for this
- Master Data Management is the process of normalizing data before it enters the data warehouse
- Master Data Management and data governance go hand in hand
    - compliance with regulations and mandates
    - enforcing data integrity and data quality
    - streamlining the access to data
    - automatically populating datasets consistently

## Streamlining Data

- Consolidation of multiple fields, to fill in gaps in data via two different systems
- Field standardization, process of standardizing records, like names, states, country abbreviations and such
- Data Dictionary, a record of what is in the master data management
    - provides quick access to available data and how it relates to other data in the organization

