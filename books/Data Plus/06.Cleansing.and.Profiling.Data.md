# Cleansing and Profiling Data

- Data Profiling: process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects
- Can be used to determine data quality issues and what needs to be corrected in ETL

## Data Profiling Steps

1. Collecting descriptive statistics like min, max, count and sum
2. Collecting data types, length and recurring patterns
3. Tagging data with keywords, descriptions or categories
4. Performing data quality assessment, risk of performing joins on the data
5. Discovering metadata and assessing its accuracy
6. Identifying distributions, key candidates, foreign-key candidates, functional dependencies, embedded value dependencies, and performing inter-table analysis

## Data Profiling Tools

- Allow both manual techniques and advanced software to start data profiling
    - Power Query in Excel
	- Power BI
	- Tableau

## Redundant and Duplicated Data

- Redundant Data, stored in more than one place (multiple columns or tables)
    - Work on consolidating data to minimize redundancy
	- Can be removed without losing information since it can be derived elsewhere in the database
- Duplicated Data, data repeated within the same dataset

## Unnecessary Data

- We don't always need all the data for our analysis
- Only record data that's needed to answer business questions
- Extra data slows down the data systems
- Can be referred to as noise

## Missing Data

- When a field contains, N/A, Null, or is just an empty field
    1. When the value is not applicable to the field
	2. When the dataset doesn't have that information
	3. When the datasets do not match the expected information
	4. When survey data is incomplete
- Filter out NULL values OR replace the Null values

## Invalid Data

- Any data that is incorrect due to:
    - Hard coded data that hasn't been updated
	- Invalid data questions
	- Extreme values and outliers
	- Just plain wrong data
	- Invisible and non-printable characters
- Look for leading and trailing spaces
- Remove/replace invalid data

## Meeting Specifications

- Certain types of quality standards set by database engineers when designing system must be met
- Most common reason that data doesn't meet specifications is wrong data type
- Another reason, improper storage of numeric characters
    - Make sure data conversions are happening during import

## Data Outliers

- Any data that is far outside the normal distance of other values in the sample
- Nonparametric Statistics, identifies data not assumed to come from a prescribed model that are predetermined by a small number of parameters
- Parametric, based on assumptions about the distribution of data, example is Student's t-tests
- Non-parametric, not based on assumptions, example is Wilcoxon test

